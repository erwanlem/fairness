{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In processing `Adversarial debiasing`\n",
    "\n",
    "Ce fichier est dédié à la correction de biais in-processing de notre modèle.\n",
    "La méthode utilisée est `Adversarial debiasing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:07:22.603366: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-09 12:07:22.849438: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-09 12:07:22.849496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-09 12:07:22.886928: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-09 12:07:22.972305: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-09 12:07:23.824838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/erwan/.local/lib/python3.10/site-packages/torch/_functorch/deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset_prepare import *\n",
    "from utils import custom_RFC, print_metrics\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets.standard_dataset import StandardDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing, DisparateImpactRemover\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from sklearn.metrics import classification_report\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_Acc</th>\n",
       "      <th>trajet</th>\n",
       "      <th>catr</th>\n",
       "      <th>circ</th>\n",
       "      <th>nbv</th>\n",
       "      <th>prof</th>\n",
       "      <th>plan</th>\n",
       "      <th>surf</th>\n",
       "      <th>infra</th>\n",
       "      <th>situ</th>\n",
       "      <th>...</th>\n",
       "      <th>atm</th>\n",
       "      <th>col</th>\n",
       "      <th>catv</th>\n",
       "      <th>obs</th>\n",
       "      <th>obsm</th>\n",
       "      <th>choc</th>\n",
       "      <th>mortal</th>\n",
       "      <th>pieton</th>\n",
       "      <th>sexe_conducteur</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202200000001</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202200000001</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202200000002</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202200000002</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202200000003</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126656</th>\n",
       "      <td>202200055300</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126657</th>\n",
       "      <td>202200055301</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126659</th>\n",
       "      <td>202200055301</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126660</th>\n",
       "      <td>202200055302</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126661</th>\n",
       "      <td>202200055302</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92614 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Num_Acc  trajet  catr  circ  nbv  prof  plan  surf  infra  situ  \\\n",
       "0       202200000001       5     4     2    2     1     1     1      0     1   \n",
       "1       202200000001       5     4     2    2     1     1     1      0     1   \n",
       "2       202200000002       0     4     2    2     1     1     1      0     1   \n",
       "3       202200000002       4     4     2    2     1     1     1      0     1   \n",
       "4       202200000003       0     3    -1    2     1     1     1      5     1   \n",
       "...              ...     ...   ...   ...  ...   ...   ...   ...    ...   ...   \n",
       "126656  202200055300       5     3     2    2     1     2     7      0     3   \n",
       "126657  202200055301       5     3     2    2     1     1     1      0     1   \n",
       "126659  202200055301       5     3     2    2     1     1     1      0     1   \n",
       "126660  202200055302       1     3     3    4     1     1     1      0     1   \n",
       "126661  202200055302       0     3     3    4     1     1     1      0     1   \n",
       "\n",
       "        ...  atm  col  catv  obs  obsm  choc  mortal  pieton  sexe_conducteur  \\\n",
       "0       ...    1    3     2    0     2     1       0       0                1   \n",
       "1       ...    1    3     2    0     2     1       0       0                1   \n",
       "2       ...    1    3     3    0     2     8       0       0                1   \n",
       "3       ...    1    3     3    0     2     8       0       0                1   \n",
       "4       ...    1    2     3    0     2     1       0       0                1   \n",
       "...     ...  ...  ...   ...  ...   ...   ...     ...     ...              ...   \n",
       "126656  ...    9    6     3    1     0     1       1       0                1   \n",
       "126657  ...    1    3     3    0     0     8       0       0                0   \n",
       "126659  ...    1    3     3    0     0     8       0       0                0   \n",
       "126660  ...    1    2     5    0     2     1       0       0                1   \n",
       "126661  ...    1    2     5    0     2     1       0       0                1   \n",
       "\n",
       "        age  \n",
       "0        14  \n",
       "1        74  \n",
       "2        34  \n",
       "3        52  \n",
       "4        20  \n",
       "...     ...  \n",
       "126656   27  \n",
       "126657   20  \n",
       "126659   69  \n",
       "126660   30  \n",
       "126661   22  \n",
       "\n",
       "[92614 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'mortal'\n",
    "\n",
    "# Récupération des ensembles de train/test\n",
    "X_train, X_test, y_train, y_test = test_train_sets(df)\n",
    "df = df.drop(columns='Num_Acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_test = prepare_standard_dataset(X_train, y_train, X_test, y_test, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups=[{'sexe_conducteur': 1}]\n",
    "unprivileged_groups = [{'sexe_conducteur' : 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/erwan/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "epoch 0; iter: 0; batch classifier loss: 3.902865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:07:39.953993: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 200; batch classifier loss: 2.167028\n",
      "epoch 0; iter: 400; batch classifier loss: 2.065169\n",
      "epoch 0; iter: 600; batch classifier loss: 1.244217\n",
      "epoch 0; iter: 800; batch classifier loss: 0.570744\n",
      "epoch 1; iter: 0; batch classifier loss: 0.654130\n",
      "epoch 1; iter: 200; batch classifier loss: 0.594497\n",
      "epoch 1; iter: 400; batch classifier loss: 0.609645\n",
      "epoch 1; iter: 600; batch classifier loss: 0.603588\n",
      "epoch 1; iter: 800; batch classifier loss: 0.425337\n",
      "epoch 2; iter: 0; batch classifier loss: 0.441410\n",
      "epoch 2; iter: 200; batch classifier loss: 0.551197\n",
      "epoch 2; iter: 400; batch classifier loss: 0.398407\n",
      "epoch 2; iter: 600; batch classifier loss: 0.426612\n",
      "epoch 2; iter: 800; batch classifier loss: 0.478975\n",
      "epoch 3; iter: 0; batch classifier loss: 0.440024\n",
      "epoch 3; iter: 200; batch classifier loss: 0.437620\n",
      "epoch 3; iter: 400; batch classifier loss: 0.377839\n",
      "epoch 3; iter: 600; batch classifier loss: 0.345303\n",
      "epoch 3; iter: 800; batch classifier loss: 0.474725\n",
      "epoch 4; iter: 0; batch classifier loss: 0.410107\n",
      "epoch 4; iter: 200; batch classifier loss: 0.498924\n",
      "epoch 4; iter: 400; batch classifier loss: 0.441570\n",
      "epoch 4; iter: 600; batch classifier loss: 0.393903\n",
      "epoch 4; iter: 800; batch classifier loss: 0.407457\n",
      "epoch 5; iter: 0; batch classifier loss: 0.373321\n",
      "epoch 5; iter: 200; batch classifier loss: 0.378807\n",
      "epoch 5; iter: 400; batch classifier loss: 0.428279\n",
      "epoch 5; iter: 600; batch classifier loss: 0.466586\n",
      "epoch 5; iter: 800; batch classifier loss: 0.451271\n",
      "epoch 6; iter: 0; batch classifier loss: 0.429011\n",
      "epoch 6; iter: 200; batch classifier loss: 0.400854\n",
      "epoch 6; iter: 400; batch classifier loss: 0.446567\n",
      "epoch 6; iter: 600; batch classifier loss: 0.340163\n",
      "epoch 6; iter: 800; batch classifier loss: 0.337194\n",
      "epoch 7; iter: 0; batch classifier loss: 0.432560\n",
      "epoch 7; iter: 200; batch classifier loss: 0.428109\n",
      "epoch 7; iter: 400; batch classifier loss: 0.413872\n",
      "epoch 7; iter: 600; batch classifier loss: 0.461944\n",
      "epoch 7; iter: 800; batch classifier loss: 0.465584\n",
      "epoch 8; iter: 0; batch classifier loss: 0.363924\n",
      "epoch 8; iter: 200; batch classifier loss: 0.368835\n",
      "epoch 8; iter: 400; batch classifier loss: 0.370021\n",
      "epoch 8; iter: 600; batch classifier loss: 0.375027\n",
      "epoch 8; iter: 800; batch classifier loss: 0.355228\n",
      "epoch 9; iter: 0; batch classifier loss: 0.408433\n",
      "epoch 9; iter: 200; batch classifier loss: 0.391439\n",
      "epoch 9; iter: 400; batch classifier loss: 0.435830\n",
      "epoch 9; iter: 600; batch classifier loss: 0.374877\n",
      "epoch 9; iter: 800; batch classifier loss: 0.476622\n",
      "epoch 10; iter: 0; batch classifier loss: 0.485826\n",
      "epoch 10; iter: 200; batch classifier loss: 0.452833\n",
      "epoch 10; iter: 400; batch classifier loss: 0.426219\n",
      "epoch 10; iter: 600; batch classifier loss: 0.411135\n",
      "epoch 10; iter: 800; batch classifier loss: 0.298845\n",
      "epoch 11; iter: 0; batch classifier loss: 0.368295\n",
      "epoch 11; iter: 200; batch classifier loss: 0.433418\n",
      "epoch 11; iter: 400; batch classifier loss: 0.518882\n",
      "epoch 11; iter: 600; batch classifier loss: 0.361975\n",
      "epoch 11; iter: 800; batch classifier loss: 0.353931\n",
      "epoch 12; iter: 0; batch classifier loss: 0.358157\n",
      "epoch 12; iter: 200; batch classifier loss: 0.330308\n",
      "epoch 12; iter: 400; batch classifier loss: 0.349619\n",
      "epoch 12; iter: 600; batch classifier loss: 0.335039\n",
      "epoch 12; iter: 800; batch classifier loss: 0.458391\n",
      "epoch 13; iter: 0; batch classifier loss: 0.400678\n",
      "epoch 13; iter: 200; batch classifier loss: 0.338823\n",
      "epoch 13; iter: 400; batch classifier loss: 0.406733\n",
      "epoch 13; iter: 600; batch classifier loss: 0.314263\n",
      "epoch 13; iter: 800; batch classifier loss: 0.278775\n",
      "epoch 14; iter: 0; batch classifier loss: 0.349135\n",
      "epoch 14; iter: 200; batch classifier loss: 0.360241\n",
      "epoch 14; iter: 400; batch classifier loss: 0.264469\n",
      "epoch 14; iter: 600; batch classifier loss: 0.343527\n",
      "epoch 14; iter: 800; batch classifier loss: 0.344097\n",
      "epoch 15; iter: 0; batch classifier loss: 0.401184\n",
      "epoch 15; iter: 200; batch classifier loss: 0.356767\n",
      "epoch 15; iter: 400; batch classifier loss: 0.302006\n",
      "epoch 15; iter: 600; batch classifier loss: 0.430812\n",
      "epoch 15; iter: 800; batch classifier loss: 0.436401\n",
      "epoch 16; iter: 0; batch classifier loss: 0.349263\n",
      "epoch 16; iter: 200; batch classifier loss: 0.315290\n",
      "epoch 16; iter: 400; batch classifier loss: 0.367821\n",
      "epoch 16; iter: 600; batch classifier loss: 0.337844\n",
      "epoch 16; iter: 800; batch classifier loss: 0.336559\n",
      "epoch 17; iter: 0; batch classifier loss: 0.405156\n",
      "epoch 17; iter: 200; batch classifier loss: 0.369179\n",
      "epoch 17; iter: 400; batch classifier loss: 0.339430\n",
      "epoch 17; iter: 600; batch classifier loss: 0.331704\n",
      "epoch 17; iter: 800; batch classifier loss: 0.341932\n",
      "epoch 18; iter: 0; batch classifier loss: 0.293001\n",
      "epoch 18; iter: 200; batch classifier loss: 0.407113\n",
      "epoch 18; iter: 400; batch classifier loss: 0.375351\n",
      "epoch 18; iter: 600; batch classifier loss: 0.325648\n",
      "epoch 18; iter: 800; batch classifier loss: 0.418671\n",
      "epoch 19; iter: 0; batch classifier loss: 0.322032\n",
      "epoch 19; iter: 200; batch classifier loss: 0.367823\n",
      "epoch 19; iter: 400; batch classifier loss: 0.367118\n",
      "epoch 19; iter: 600; batch classifier loss: 0.341003\n",
      "epoch 19; iter: 800; batch classifier loss: 0.358095\n",
      "epoch 20; iter: 0; batch classifier loss: 0.349968\n",
      "epoch 20; iter: 200; batch classifier loss: 0.346081\n",
      "epoch 20; iter: 400; batch classifier loss: 0.346744\n",
      "epoch 20; iter: 600; batch classifier loss: 0.238493\n",
      "epoch 20; iter: 800; batch classifier loss: 0.349998\n",
      "epoch 21; iter: 0; batch classifier loss: 0.274618\n",
      "epoch 21; iter: 200; batch classifier loss: 0.294211\n",
      "epoch 21; iter: 400; batch classifier loss: 0.281450\n",
      "epoch 21; iter: 600; batch classifier loss: 0.353324\n",
      "epoch 21; iter: 800; batch classifier loss: 0.325734\n",
      "epoch 22; iter: 0; batch classifier loss: 0.288296\n",
      "epoch 22; iter: 200; batch classifier loss: 0.324635\n",
      "epoch 22; iter: 400; batch classifier loss: 0.395749\n",
      "epoch 22; iter: 600; batch classifier loss: 0.207740\n",
      "epoch 22; iter: 800; batch classifier loss: 0.327704\n",
      "epoch 23; iter: 0; batch classifier loss: 0.388450\n",
      "epoch 23; iter: 200; batch classifier loss: 0.300579\n",
      "epoch 23; iter: 400; batch classifier loss: 0.310681\n",
      "epoch 23; iter: 600; batch classifier loss: 0.319582\n",
      "epoch 23; iter: 800; batch classifier loss: 0.313086\n",
      "epoch 24; iter: 0; batch classifier loss: 0.339299\n",
      "epoch 24; iter: 200; batch classifier loss: 0.380951\n",
      "epoch 24; iter: 400; batch classifier loss: 0.357239\n",
      "epoch 24; iter: 600; batch classifier loss: 0.304050\n",
      "epoch 24; iter: 800; batch classifier loss: 0.403301\n",
      "epoch 25; iter: 0; batch classifier loss: 0.331184\n",
      "epoch 25; iter: 200; batch classifier loss: 0.239494\n",
      "epoch 25; iter: 400; batch classifier loss: 0.446820\n",
      "epoch 25; iter: 600; batch classifier loss: 0.391447\n",
      "epoch 25; iter: 800; batch classifier loss: 0.344133\n",
      "epoch 26; iter: 0; batch classifier loss: 0.292109\n",
      "epoch 26; iter: 200; batch classifier loss: 0.252377\n",
      "epoch 26; iter: 400; batch classifier loss: 0.344457\n",
      "epoch 26; iter: 600; batch classifier loss: 0.334991\n",
      "epoch 26; iter: 800; batch classifier loss: 0.293514\n",
      "epoch 27; iter: 0; batch classifier loss: 0.276139\n",
      "epoch 27; iter: 200; batch classifier loss: 0.382955\n",
      "epoch 27; iter: 400; batch classifier loss: 0.411663\n",
      "epoch 27; iter: 600; batch classifier loss: 0.276382\n",
      "epoch 27; iter: 800; batch classifier loss: 0.340277\n",
      "epoch 28; iter: 0; batch classifier loss: 0.458661\n",
      "epoch 28; iter: 200; batch classifier loss: 0.367180\n",
      "epoch 28; iter: 400; batch classifier loss: 0.329894\n",
      "epoch 28; iter: 600; batch classifier loss: 0.340470\n",
      "epoch 28; iter: 800; batch classifier loss: 0.255675\n",
      "epoch 29; iter: 0; batch classifier loss: 0.381773\n",
      "epoch 29; iter: 200; batch classifier loss: 0.322891\n",
      "epoch 29; iter: 400; batch classifier loss: 0.286404\n",
      "epoch 29; iter: 600; batch classifier loss: 0.339028\n",
      "epoch 29; iter: 800; batch classifier loss: 0.305666\n",
      "epoch 30; iter: 0; batch classifier loss: 0.454941\n",
      "epoch 30; iter: 200; batch classifier loss: 0.266474\n",
      "epoch 30; iter: 400; batch classifier loss: 0.334542\n",
      "epoch 30; iter: 600; batch classifier loss: 0.271566\n",
      "epoch 30; iter: 800; batch classifier loss: 0.246325\n",
      "epoch 31; iter: 0; batch classifier loss: 0.263175\n",
      "epoch 31; iter: 200; batch classifier loss: 0.276922\n",
      "epoch 31; iter: 400; batch classifier loss: 0.300728\n",
      "epoch 31; iter: 600; batch classifier loss: 0.306855\n",
      "epoch 31; iter: 800; batch classifier loss: 0.318020\n",
      "epoch 32; iter: 0; batch classifier loss: 0.297452\n",
      "epoch 32; iter: 200; batch classifier loss: 0.304351\n",
      "epoch 32; iter: 400; batch classifier loss: 0.455015\n",
      "epoch 32; iter: 600; batch classifier loss: 0.292259\n",
      "epoch 32; iter: 800; batch classifier loss: 0.274809\n",
      "epoch 33; iter: 0; batch classifier loss: 0.374741\n",
      "epoch 33; iter: 200; batch classifier loss: 0.352776\n",
      "epoch 33; iter: 400; batch classifier loss: 0.258139\n",
      "epoch 33; iter: 600; batch classifier loss: 0.215850\n",
      "epoch 33; iter: 800; batch classifier loss: 0.257241\n",
      "epoch 34; iter: 0; batch classifier loss: 0.286723\n",
      "epoch 34; iter: 200; batch classifier loss: 0.360708\n",
      "epoch 34; iter: 400; batch classifier loss: 0.315160\n",
      "epoch 34; iter: 600; batch classifier loss: 0.291381\n",
      "epoch 34; iter: 800; batch classifier loss: 0.296669\n",
      "epoch 35; iter: 0; batch classifier loss: 0.394189\n",
      "epoch 35; iter: 200; batch classifier loss: 0.249853\n",
      "epoch 35; iter: 400; batch classifier loss: 0.380019\n",
      "epoch 35; iter: 600; batch classifier loss: 0.339962\n",
      "epoch 35; iter: 800; batch classifier loss: 0.261148\n",
      "epoch 36; iter: 0; batch classifier loss: 0.264467\n",
      "epoch 36; iter: 200; batch classifier loss: 0.352483\n",
      "epoch 36; iter: 400; batch classifier loss: 0.283915\n",
      "epoch 36; iter: 600; batch classifier loss: 0.308270\n",
      "epoch 36; iter: 800; batch classifier loss: 0.290191\n",
      "epoch 37; iter: 0; batch classifier loss: 0.261275\n",
      "epoch 37; iter: 200; batch classifier loss: 0.337418\n",
      "epoch 37; iter: 400; batch classifier loss: 0.307458\n",
      "epoch 37; iter: 600; batch classifier loss: 0.309802\n",
      "epoch 37; iter: 800; batch classifier loss: 0.276911\n",
      "epoch 38; iter: 0; batch classifier loss: 0.323486\n",
      "epoch 38; iter: 200; batch classifier loss: 0.328661\n",
      "epoch 38; iter: 400; batch classifier loss: 0.264627\n",
      "epoch 38; iter: 600; batch classifier loss: 0.302389\n",
      "epoch 38; iter: 800; batch classifier loss: 0.250989\n",
      "epoch 39; iter: 0; batch classifier loss: 0.358955\n",
      "epoch 39; iter: 200; batch classifier loss: 0.303868\n",
      "epoch 39; iter: 400; batch classifier loss: 0.311261\n",
      "epoch 39; iter: 600; batch classifier loss: 0.246461\n",
      "epoch 39; iter: 800; batch classifier loss: 0.363562\n",
      "epoch 40; iter: 0; batch classifier loss: 0.325963\n",
      "epoch 40; iter: 200; batch classifier loss: 0.279927\n",
      "epoch 40; iter: 400; batch classifier loss: 0.357486\n",
      "epoch 40; iter: 600; batch classifier loss: 0.402712\n",
      "epoch 40; iter: 800; batch classifier loss: 0.311429\n",
      "epoch 41; iter: 0; batch classifier loss: 0.281530\n",
      "epoch 41; iter: 200; batch classifier loss: 0.369057\n",
      "epoch 41; iter: 400; batch classifier loss: 0.293867\n",
      "epoch 41; iter: 600; batch classifier loss: 0.330844\n",
      "epoch 41; iter: 800; batch classifier loss: 0.336344\n",
      "epoch 42; iter: 0; batch classifier loss: 0.278726\n",
      "epoch 42; iter: 200; batch classifier loss: 0.280595\n",
      "epoch 42; iter: 400; batch classifier loss: 0.273432\n",
      "epoch 42; iter: 600; batch classifier loss: 0.274247\n",
      "epoch 42; iter: 800; batch classifier loss: 0.251612\n",
      "epoch 43; iter: 0; batch classifier loss: 0.375626\n",
      "epoch 43; iter: 200; batch classifier loss: 0.285741\n",
      "epoch 43; iter: 400; batch classifier loss: 0.312101\n",
      "epoch 43; iter: 600; batch classifier loss: 0.283334\n",
      "epoch 43; iter: 800; batch classifier loss: 0.394379\n",
      "epoch 44; iter: 0; batch classifier loss: 0.380874\n",
      "epoch 44; iter: 200; batch classifier loss: 0.293815\n",
      "epoch 44; iter: 400; batch classifier loss: 0.245143\n",
      "epoch 44; iter: 600; batch classifier loss: 0.347426\n",
      "epoch 44; iter: 800; batch classifier loss: 0.303855\n",
      "epoch 45; iter: 0; batch classifier loss: 0.368536\n",
      "epoch 45; iter: 200; batch classifier loss: 0.279997\n",
      "epoch 45; iter: 400; batch classifier loss: 0.249400\n",
      "epoch 45; iter: 600; batch classifier loss: 0.287659\n",
      "epoch 45; iter: 800; batch classifier loss: 0.281411\n",
      "epoch 46; iter: 0; batch classifier loss: 0.295446\n",
      "epoch 46; iter: 200; batch classifier loss: 0.375399\n",
      "epoch 46; iter: 400; batch classifier loss: 0.360739\n",
      "epoch 46; iter: 600; batch classifier loss: 0.309639\n",
      "epoch 46; iter: 800; batch classifier loss: 0.279784\n",
      "epoch 47; iter: 0; batch classifier loss: 0.262137\n",
      "epoch 47; iter: 200; batch classifier loss: 0.303395\n",
      "epoch 47; iter: 400; batch classifier loss: 0.315471\n",
      "epoch 47; iter: 600; batch classifier loss: 0.323432\n",
      "epoch 47; iter: 800; batch classifier loss: 0.317139\n",
      "epoch 48; iter: 0; batch classifier loss: 0.340725\n",
      "epoch 48; iter: 200; batch classifier loss: 0.287962\n",
      "epoch 48; iter: 400; batch classifier loss: 0.316550\n",
      "epoch 48; iter: 600; batch classifier loss: 0.437870\n",
      "epoch 48; iter: 800; batch classifier loss: 0.361991\n",
      "epoch 49; iter: 0; batch classifier loss: 0.373027\n",
      "epoch 49; iter: 200; batch classifier loss: 0.434273\n",
      "epoch 49; iter: 400; batch classifier loss: 0.273251\n",
      "epoch 49; iter: 600; batch classifier loss: 0.347405\n",
      "epoch 49; iter: 800; batch classifier loss: 0.340090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7fa504c8f3a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "sess = tf.Session()\n",
    "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess)\n",
    "plain_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.085004\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.014355\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.823874\n",
      "Test set: Balanced classification accuracy = 0.633825\n",
      "Test set: Disparate impact = 1.087832\n",
      "Test set: Equal opportunity difference = 0.080551\n",
      "Test set: Average odds difference = 0.049111\n",
      "Test set: Theil_index = 0.073639\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(dataset_nodebiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(dataset_nodebiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_nodebiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 4.954613; batch adversarial loss: 0.676279\n",
      "epoch 0; iter: 200; batch classifier loss: 1.373940; batch adversarial loss: 0.646117\n",
      "epoch 0; iter: 400; batch classifier loss: 0.901361; batch adversarial loss: 0.593258\n",
      "epoch 0; iter: 600; batch classifier loss: 0.723112; batch adversarial loss: 0.608463\n",
      "epoch 0; iter: 800; batch classifier loss: 0.578914; batch adversarial loss: 0.557362\n",
      "epoch 1; iter: 0; batch classifier loss: 0.621347; batch adversarial loss: 0.617289\n",
      "epoch 1; iter: 200; batch classifier loss: 0.563149; batch adversarial loss: 0.534972\n",
      "epoch 1; iter: 400; batch classifier loss: 0.545843; batch adversarial loss: 0.614904\n",
      "epoch 1; iter: 600; batch classifier loss: 0.527005; batch adversarial loss: 0.607454\n",
      "epoch 1; iter: 800; batch classifier loss: 0.492137; batch adversarial loss: 0.559514\n",
      "epoch 2; iter: 0; batch classifier loss: 0.458989; batch adversarial loss: 0.629538\n",
      "epoch 2; iter: 200; batch classifier loss: 0.579382; batch adversarial loss: 0.581525\n",
      "epoch 2; iter: 400; batch classifier loss: 0.638884; batch adversarial loss: 0.576215\n",
      "epoch 2; iter: 600; batch classifier loss: 0.461912; batch adversarial loss: 0.576516\n",
      "epoch 2; iter: 800; batch classifier loss: 0.448530; batch adversarial loss: 0.638310\n",
      "epoch 3; iter: 0; batch classifier loss: 0.521407; batch adversarial loss: 0.582587\n",
      "epoch 3; iter: 200; batch classifier loss: 0.531562; batch adversarial loss: 0.558587\n",
      "epoch 3; iter: 400; batch classifier loss: 0.535621; batch adversarial loss: 0.672638\n",
      "epoch 3; iter: 600; batch classifier loss: 0.569885; batch adversarial loss: 0.590584\n",
      "epoch 3; iter: 800; batch classifier loss: 0.362104; batch adversarial loss: 0.596775\n",
      "epoch 4; iter: 0; batch classifier loss: 0.485124; batch adversarial loss: 0.604413\n",
      "epoch 4; iter: 200; batch classifier loss: 0.505469; batch adversarial loss: 0.623120\n",
      "epoch 4; iter: 400; batch classifier loss: 0.432891; batch adversarial loss: 0.567015\n",
      "epoch 4; iter: 600; batch classifier loss: 0.384920; batch adversarial loss: 0.595983\n",
      "epoch 4; iter: 800; batch classifier loss: 0.425657; batch adversarial loss: 0.644562\n",
      "epoch 5; iter: 0; batch classifier loss: 0.428168; batch adversarial loss: 0.590584\n",
      "epoch 5; iter: 200; batch classifier loss: 0.454554; batch adversarial loss: 0.661104\n",
      "epoch 5; iter: 400; batch classifier loss: 0.438130; batch adversarial loss: 0.542380\n",
      "epoch 5; iter: 600; batch classifier loss: 0.362053; batch adversarial loss: 0.609569\n",
      "epoch 5; iter: 800; batch classifier loss: 0.392771; batch adversarial loss: 0.592038\n",
      "epoch 6; iter: 0; batch classifier loss: 0.592720; batch adversarial loss: 0.604511\n",
      "epoch 6; iter: 200; batch classifier loss: 0.409648; batch adversarial loss: 0.613718\n",
      "epoch 6; iter: 400; batch classifier loss: 0.413559; batch adversarial loss: 0.652184\n",
      "epoch 6; iter: 600; batch classifier loss: 0.387498; batch adversarial loss: 0.641940\n",
      "epoch 6; iter: 800; batch classifier loss: 0.425776; batch adversarial loss: 0.646221\n",
      "epoch 7; iter: 0; batch classifier loss: 0.421875; batch adversarial loss: 0.653102\n",
      "epoch 7; iter: 200; batch classifier loss: 0.339229; batch adversarial loss: 0.572999\n",
      "epoch 7; iter: 400; batch classifier loss: 0.458884; batch adversarial loss: 0.559259\n",
      "epoch 7; iter: 600; batch classifier loss: 0.383342; batch adversarial loss: 0.580597\n",
      "epoch 7; iter: 800; batch classifier loss: 0.438719; batch adversarial loss: 0.644456\n",
      "epoch 8; iter: 0; batch classifier loss: 0.426301; batch adversarial loss: 0.563840\n",
      "epoch 8; iter: 200; batch classifier loss: 0.401278; batch adversarial loss: 0.617983\n",
      "epoch 8; iter: 400; batch classifier loss: 0.356347; batch adversarial loss: 0.612386\n",
      "epoch 8; iter: 600; batch classifier loss: 0.425806; batch adversarial loss: 0.590741\n",
      "epoch 8; iter: 800; batch classifier loss: 0.438874; batch adversarial loss: 0.650020\n",
      "epoch 9; iter: 0; batch classifier loss: 0.376843; batch adversarial loss: 0.562431\n",
      "epoch 9; iter: 200; batch classifier loss: 0.349082; batch adversarial loss: 0.598150\n",
      "epoch 9; iter: 400; batch classifier loss: 0.377595; batch adversarial loss: 0.617600\n",
      "epoch 9; iter: 600; batch classifier loss: 0.386001; batch adversarial loss: 0.578717\n",
      "epoch 9; iter: 800; batch classifier loss: 0.376358; batch adversarial loss: 0.607831\n",
      "epoch 10; iter: 0; batch classifier loss: 0.416298; batch adversarial loss: 0.543229\n",
      "epoch 10; iter: 200; batch classifier loss: 0.424482; batch adversarial loss: 0.644199\n",
      "epoch 10; iter: 400; batch classifier loss: 0.532121; batch adversarial loss: 0.664058\n",
      "epoch 10; iter: 600; batch classifier loss: 0.377037; batch adversarial loss: 0.588400\n",
      "epoch 10; iter: 800; batch classifier loss: 0.413744; batch adversarial loss: 0.607511\n",
      "epoch 11; iter: 0; batch classifier loss: 0.403063; batch adversarial loss: 0.659572\n",
      "epoch 11; iter: 200; batch classifier loss: 0.487209; batch adversarial loss: 0.544440\n",
      "epoch 11; iter: 400; batch classifier loss: 0.347304; batch adversarial loss: 0.570524\n",
      "epoch 11; iter: 600; batch classifier loss: 0.412552; batch adversarial loss: 0.624300\n",
      "epoch 11; iter: 800; batch classifier loss: 0.473421; batch adversarial loss: 0.640940\n",
      "epoch 12; iter: 0; batch classifier loss: 0.375054; batch adversarial loss: 0.601402\n",
      "epoch 12; iter: 200; batch classifier loss: 0.386947; batch adversarial loss: 0.605777\n",
      "epoch 12; iter: 400; batch classifier loss: 0.369974; batch adversarial loss: 0.592456\n",
      "epoch 12; iter: 600; batch classifier loss: 0.392001; batch adversarial loss: 0.646289\n",
      "epoch 12; iter: 800; batch classifier loss: 0.371562; batch adversarial loss: 0.608986\n",
      "epoch 13; iter: 0; batch classifier loss: 0.374081; batch adversarial loss: 0.638930\n",
      "epoch 13; iter: 200; batch classifier loss: 0.375587; batch adversarial loss: 0.618591\n",
      "epoch 13; iter: 400; batch classifier loss: 0.357645; batch adversarial loss: 0.601830\n",
      "epoch 13; iter: 600; batch classifier loss: 0.430376; batch adversarial loss: 0.592250\n",
      "epoch 13; iter: 800; batch classifier loss: 0.355381; batch adversarial loss: 0.642824\n",
      "epoch 14; iter: 0; batch classifier loss: 0.368865; batch adversarial loss: 0.635667\n",
      "epoch 14; iter: 200; batch classifier loss: 0.340380; batch adversarial loss: 0.592352\n",
      "epoch 14; iter: 400; batch classifier loss: 0.394384; batch adversarial loss: 0.604221\n",
      "epoch 14; iter: 600; batch classifier loss: 0.375907; batch adversarial loss: 0.553616\n",
      "epoch 14; iter: 800; batch classifier loss: 0.443979; batch adversarial loss: 0.572447\n",
      "epoch 15; iter: 0; batch classifier loss: 0.400077; batch adversarial loss: 0.612396\n",
      "epoch 15; iter: 200; batch classifier loss: 0.416532; batch adversarial loss: 0.660685\n",
      "epoch 15; iter: 400; batch classifier loss: 0.390792; batch adversarial loss: 0.649244\n",
      "epoch 15; iter: 600; batch classifier loss: 0.465425; batch adversarial loss: 0.622433\n",
      "epoch 15; iter: 800; batch classifier loss: 0.358579; batch adversarial loss: 0.619400\n",
      "epoch 16; iter: 0; batch classifier loss: 0.345091; batch adversarial loss: 0.626805\n",
      "epoch 16; iter: 200; batch classifier loss: 0.296320; batch adversarial loss: 0.605508\n",
      "epoch 16; iter: 400; batch classifier loss: 0.338595; batch adversarial loss: 0.575106\n",
      "epoch 16; iter: 600; batch classifier loss: 0.302754; batch adversarial loss: 0.613490\n",
      "epoch 16; iter: 800; batch classifier loss: 0.320216; batch adversarial loss: 0.616886\n",
      "epoch 17; iter: 0; batch classifier loss: 0.388555; batch adversarial loss: 0.612264\n",
      "epoch 17; iter: 200; batch classifier loss: 0.403644; batch adversarial loss: 0.658378\n",
      "epoch 17; iter: 400; batch classifier loss: 0.468006; batch adversarial loss: 0.641775\n",
      "epoch 17; iter: 600; batch classifier loss: 0.383342; batch adversarial loss: 0.636612\n",
      "epoch 17; iter: 800; batch classifier loss: 0.315848; batch adversarial loss: 0.600890\n",
      "epoch 18; iter: 0; batch classifier loss: 0.390568; batch adversarial loss: 0.675611\n",
      "epoch 18; iter: 200; batch classifier loss: 0.347993; batch adversarial loss: 0.592833\n",
      "epoch 18; iter: 400; batch classifier loss: 0.338507; batch adversarial loss: 0.599835\n",
      "epoch 18; iter: 600; batch classifier loss: 0.311132; batch adversarial loss: 0.669664\n",
      "epoch 18; iter: 800; batch classifier loss: 0.318550; batch adversarial loss: 0.563866\n",
      "epoch 19; iter: 0; batch classifier loss: 0.455449; batch adversarial loss: 0.682188\n",
      "epoch 19; iter: 200; batch classifier loss: 0.370426; batch adversarial loss: 0.601847\n",
      "epoch 19; iter: 400; batch classifier loss: 0.333929; batch adversarial loss: 0.549896\n",
      "epoch 19; iter: 600; batch classifier loss: 0.371380; batch adversarial loss: 0.584642\n",
      "epoch 19; iter: 800; batch classifier loss: 0.416521; batch adversarial loss: 0.613468\n",
      "epoch 20; iter: 0; batch classifier loss: 0.341533; batch adversarial loss: 0.642575\n",
      "epoch 20; iter: 200; batch classifier loss: 0.316985; batch adversarial loss: 0.539300\n",
      "epoch 20; iter: 400; batch classifier loss: 0.342528; batch adversarial loss: 0.613669\n",
      "epoch 20; iter: 600; batch classifier loss: 0.292154; batch adversarial loss: 0.549117\n",
      "epoch 20; iter: 800; batch classifier loss: 0.288402; batch adversarial loss: 0.626830\n",
      "epoch 21; iter: 0; batch classifier loss: 0.321721; batch adversarial loss: 0.595282\n",
      "epoch 21; iter: 200; batch classifier loss: 0.358415; batch adversarial loss: 0.609690\n",
      "epoch 21; iter: 400; batch classifier loss: 0.423032; batch adversarial loss: 0.571881\n",
      "epoch 21; iter: 600; batch classifier loss: 0.412700; batch adversarial loss: 0.573229\n",
      "epoch 21; iter: 800; batch classifier loss: 0.369372; batch adversarial loss: 0.682600\n",
      "epoch 22; iter: 0; batch classifier loss: 0.351248; batch adversarial loss: 0.621888\n",
      "epoch 22; iter: 200; batch classifier loss: 0.359094; batch adversarial loss: 0.621451\n",
      "epoch 22; iter: 400; batch classifier loss: 0.301243; batch adversarial loss: 0.606770\n",
      "epoch 22; iter: 600; batch classifier loss: 0.270912; batch adversarial loss: 0.597022\n",
      "epoch 22; iter: 800; batch classifier loss: 0.372608; batch adversarial loss: 0.616770\n",
      "epoch 23; iter: 0; batch classifier loss: 0.275965; batch adversarial loss: 0.602468\n",
      "epoch 23; iter: 200; batch classifier loss: 0.353237; batch adversarial loss: 0.559762\n",
      "epoch 23; iter: 400; batch classifier loss: 0.391151; batch adversarial loss: 0.632271\n",
      "epoch 23; iter: 600; batch classifier loss: 0.213230; batch adversarial loss: 0.623358\n",
      "epoch 23; iter: 800; batch classifier loss: 0.438799; batch adversarial loss: 0.645061\n",
      "epoch 24; iter: 0; batch classifier loss: 0.422157; batch adversarial loss: 0.635338\n",
      "epoch 24; iter: 200; batch classifier loss: 0.303971; batch adversarial loss: 0.649987\n",
      "epoch 24; iter: 400; batch classifier loss: 0.394881; batch adversarial loss: 0.573621\n",
      "epoch 24; iter: 600; batch classifier loss: 0.245748; batch adversarial loss: 0.593030\n",
      "epoch 24; iter: 800; batch classifier loss: 0.336701; batch adversarial loss: 0.637457\n",
      "epoch 25; iter: 0; batch classifier loss: 0.319499; batch adversarial loss: 0.587074\n",
      "epoch 25; iter: 200; batch classifier loss: 0.248981; batch adversarial loss: 0.655658\n",
      "epoch 25; iter: 400; batch classifier loss: 0.343705; batch adversarial loss: 0.617516\n",
      "epoch 25; iter: 600; batch classifier loss: 0.355009; batch adversarial loss: 0.624686\n",
      "epoch 25; iter: 800; batch classifier loss: 0.312168; batch adversarial loss: 0.627066\n",
      "epoch 26; iter: 0; batch classifier loss: 0.427690; batch adversarial loss: 0.559685\n",
      "epoch 26; iter: 200; batch classifier loss: 0.311899; batch adversarial loss: 0.642647\n",
      "epoch 26; iter: 400; batch classifier loss: 0.340444; batch adversarial loss: 0.611323\n",
      "epoch 26; iter: 600; batch classifier loss: 0.384118; batch adversarial loss: 0.589578\n",
      "epoch 26; iter: 800; batch classifier loss: 0.461350; batch adversarial loss: 0.629022\n",
      "epoch 27; iter: 0; batch classifier loss: 0.268413; batch adversarial loss: 0.637875\n",
      "epoch 27; iter: 200; batch classifier loss: 0.338688; batch adversarial loss: 0.618168\n",
      "epoch 27; iter: 400; batch classifier loss: 0.359842; batch adversarial loss: 0.626858\n",
      "epoch 27; iter: 600; batch classifier loss: 0.339188; batch adversarial loss: 0.624052\n",
      "epoch 27; iter: 800; batch classifier loss: 0.404860; batch adversarial loss: 0.577948\n",
      "epoch 28; iter: 0; batch classifier loss: 0.308745; batch adversarial loss: 0.593139\n",
      "epoch 28; iter: 200; batch classifier loss: 0.330978; batch adversarial loss: 0.573047\n",
      "epoch 28; iter: 400; batch classifier loss: 0.374992; batch adversarial loss: 0.621369\n",
      "epoch 28; iter: 600; batch classifier loss: 0.345272; batch adversarial loss: 0.627900\n",
      "epoch 28; iter: 800; batch classifier loss: 0.392588; batch adversarial loss: 0.592851\n",
      "epoch 29; iter: 0; batch classifier loss: 0.316351; batch adversarial loss: 0.664068\n",
      "epoch 29; iter: 200; batch classifier loss: 0.326281; batch adversarial loss: 0.599568\n",
      "epoch 29; iter: 400; batch classifier loss: 0.321795; batch adversarial loss: 0.642324\n",
      "epoch 29; iter: 600; batch classifier loss: 0.438971; batch adversarial loss: 0.621258\n",
      "epoch 29; iter: 800; batch classifier loss: 0.277313; batch adversarial loss: 0.585387\n",
      "epoch 30; iter: 0; batch classifier loss: 0.311330; batch adversarial loss: 0.662590\n",
      "epoch 30; iter: 200; batch classifier loss: 0.323694; batch adversarial loss: 0.615001\n",
      "epoch 30; iter: 400; batch classifier loss: 0.347724; batch adversarial loss: 0.602317\n",
      "epoch 30; iter: 600; batch classifier loss: 0.353733; batch adversarial loss: 0.624764\n",
      "epoch 30; iter: 800; batch classifier loss: 0.333584; batch adversarial loss: 0.558500\n",
      "epoch 31; iter: 0; batch classifier loss: 0.294995; batch adversarial loss: 0.657284\n",
      "epoch 31; iter: 200; batch classifier loss: 0.363206; batch adversarial loss: 0.632474\n",
      "epoch 31; iter: 400; batch classifier loss: 0.337110; batch adversarial loss: 0.551068\n",
      "epoch 31; iter: 600; batch classifier loss: 0.335492; batch adversarial loss: 0.631435\n",
      "epoch 31; iter: 800; batch classifier loss: 0.339988; batch adversarial loss: 0.607697\n",
      "epoch 32; iter: 0; batch classifier loss: 0.349978; batch adversarial loss: 0.592557\n",
      "epoch 32; iter: 200; batch classifier loss: 0.371716; batch adversarial loss: 0.574516\n",
      "epoch 32; iter: 400; batch classifier loss: 0.310869; batch adversarial loss: 0.582473\n",
      "epoch 32; iter: 600; batch classifier loss: 0.335663; batch adversarial loss: 0.619366\n",
      "epoch 32; iter: 800; batch classifier loss: 0.305716; batch adversarial loss: 0.635654\n",
      "epoch 33; iter: 0; batch classifier loss: 0.362987; batch adversarial loss: 0.573060\n",
      "epoch 33; iter: 200; batch classifier loss: 0.283314; batch adversarial loss: 0.590083\n",
      "epoch 33; iter: 400; batch classifier loss: 0.317437; batch adversarial loss: 0.564120\n",
      "epoch 33; iter: 600; batch classifier loss: 0.336236; batch adversarial loss: 0.604686\n",
      "epoch 33; iter: 800; batch classifier loss: 0.369779; batch adversarial loss: 0.605428\n",
      "epoch 34; iter: 0; batch classifier loss: 0.347014; batch adversarial loss: 0.674027\n",
      "epoch 34; iter: 200; batch classifier loss: 0.378614; batch adversarial loss: 0.617335\n",
      "epoch 34; iter: 400; batch classifier loss: 0.294456; batch adversarial loss: 0.599979\n",
      "epoch 34; iter: 600; batch classifier loss: 0.274203; batch adversarial loss: 0.570000\n",
      "epoch 34; iter: 800; batch classifier loss: 0.465370; batch adversarial loss: 0.598679\n",
      "epoch 35; iter: 0; batch classifier loss: 0.355594; batch adversarial loss: 0.619861\n",
      "epoch 35; iter: 200; batch classifier loss: 0.359146; batch adversarial loss: 0.588907\n",
      "epoch 35; iter: 400; batch classifier loss: 0.391897; batch adversarial loss: 0.644361\n",
      "epoch 35; iter: 600; batch classifier loss: 0.327537; batch adversarial loss: 0.642559\n",
      "epoch 35; iter: 800; batch classifier loss: 0.368216; batch adversarial loss: 0.587652\n",
      "epoch 36; iter: 0; batch classifier loss: 0.360597; batch adversarial loss: 0.668813\n",
      "epoch 36; iter: 200; batch classifier loss: 0.400359; batch adversarial loss: 0.674003\n",
      "epoch 36; iter: 400; batch classifier loss: 0.342511; batch adversarial loss: 0.615819\n",
      "epoch 36; iter: 600; batch classifier loss: 0.269630; batch adversarial loss: 0.604056\n",
      "epoch 36; iter: 800; batch classifier loss: 0.210356; batch adversarial loss: 0.602669\n",
      "epoch 37; iter: 0; batch classifier loss: 0.266896; batch adversarial loss: 0.543066\n",
      "epoch 37; iter: 200; batch classifier loss: 0.311924; batch adversarial loss: 0.596325\n",
      "epoch 37; iter: 400; batch classifier loss: 0.307527; batch adversarial loss: 0.594979\n",
      "epoch 37; iter: 600; batch classifier loss: 0.329291; batch adversarial loss: 0.675300\n",
      "epoch 37; iter: 800; batch classifier loss: 0.369815; batch adversarial loss: 0.603658\n",
      "epoch 38; iter: 0; batch classifier loss: 0.264009; batch adversarial loss: 0.652789\n",
      "epoch 38; iter: 200; batch classifier loss: 0.283167; batch adversarial loss: 0.606722\n",
      "epoch 38; iter: 400; batch classifier loss: 0.276341; batch adversarial loss: 0.608756\n",
      "epoch 38; iter: 600; batch classifier loss: 0.340906; batch adversarial loss: 0.676598\n",
      "epoch 38; iter: 800; batch classifier loss: 0.343755; batch adversarial loss: 0.611114\n",
      "epoch 39; iter: 0; batch classifier loss: 0.393857; batch adversarial loss: 0.573821\n",
      "epoch 39; iter: 200; batch classifier loss: 0.248102; batch adversarial loss: 0.579385\n",
      "epoch 39; iter: 400; batch classifier loss: 0.396719; batch adversarial loss: 0.584296\n",
      "epoch 39; iter: 600; batch classifier loss: 0.313738; batch adversarial loss: 0.639690\n",
      "epoch 39; iter: 800; batch classifier loss: 0.444108; batch adversarial loss: 0.620610\n",
      "epoch 40; iter: 0; batch classifier loss: 0.359863; batch adversarial loss: 0.652393\n",
      "epoch 40; iter: 200; batch classifier loss: 0.315503; batch adversarial loss: 0.609731\n",
      "epoch 40; iter: 400; batch classifier loss: 0.324418; batch adversarial loss: 0.604316\n",
      "epoch 40; iter: 600; batch classifier loss: 0.375891; batch adversarial loss: 0.584499\n",
      "epoch 40; iter: 800; batch classifier loss: 0.460490; batch adversarial loss: 0.561718\n",
      "epoch 41; iter: 0; batch classifier loss: 0.286814; batch adversarial loss: 0.654174\n",
      "epoch 41; iter: 200; batch classifier loss: 0.346954; batch adversarial loss: 0.627707\n",
      "epoch 41; iter: 400; batch classifier loss: 0.298868; batch adversarial loss: 0.583544\n",
      "epoch 41; iter: 600; batch classifier loss: 0.313124; batch adversarial loss: 0.591571\n",
      "epoch 41; iter: 800; batch classifier loss: 0.455148; batch adversarial loss: 0.624278\n",
      "epoch 42; iter: 0; batch classifier loss: 0.263954; batch adversarial loss: 0.613894\n",
      "epoch 42; iter: 200; batch classifier loss: 0.205925; batch adversarial loss: 0.603899\n",
      "epoch 42; iter: 400; batch classifier loss: 0.341830; batch adversarial loss: 0.559170\n",
      "epoch 42; iter: 600; batch classifier loss: 0.288814; batch adversarial loss: 0.577118\n",
      "epoch 42; iter: 800; batch classifier loss: 0.281853; batch adversarial loss: 0.616202\n",
      "epoch 43; iter: 0; batch classifier loss: 0.313241; batch adversarial loss: 0.559483\n",
      "epoch 43; iter: 200; batch classifier loss: 0.298520; batch adversarial loss: 0.626561\n",
      "epoch 43; iter: 400; batch classifier loss: 0.381400; batch adversarial loss: 0.674623\n",
      "epoch 43; iter: 600; batch classifier loss: 0.379665; batch adversarial loss: 0.589733\n",
      "epoch 43; iter: 800; batch classifier loss: 0.370799; batch adversarial loss: 0.644048\n",
      "epoch 44; iter: 0; batch classifier loss: 0.288862; batch adversarial loss: 0.616508\n",
      "epoch 44; iter: 200; batch classifier loss: 0.320077; batch adversarial loss: 0.593099\n",
      "epoch 44; iter: 400; batch classifier loss: 0.306876; batch adversarial loss: 0.632263\n",
      "epoch 44; iter: 600; batch classifier loss: 0.292894; batch adversarial loss: 0.621394\n",
      "epoch 44; iter: 800; batch classifier loss: 0.370657; batch adversarial loss: 0.615990\n",
      "epoch 45; iter: 0; batch classifier loss: 0.294858; batch adversarial loss: 0.536938\n",
      "epoch 45; iter: 200; batch classifier loss: 0.354758; batch adversarial loss: 0.579815\n",
      "epoch 45; iter: 400; batch classifier loss: 0.316552; batch adversarial loss: 0.674019\n",
      "epoch 45; iter: 600; batch classifier loss: 0.433107; batch adversarial loss: 0.552774\n",
      "epoch 45; iter: 800; batch classifier loss: 0.367263; batch adversarial loss: 0.569519\n",
      "epoch 46; iter: 0; batch classifier loss: 0.285462; batch adversarial loss: 0.549066\n",
      "epoch 46; iter: 200; batch classifier loss: 0.277966; batch adversarial loss: 0.553411\n",
      "epoch 46; iter: 400; batch classifier loss: 0.306202; batch adversarial loss: 0.581265\n",
      "epoch 46; iter: 600; batch classifier loss: 0.340386; batch adversarial loss: 0.604428\n",
      "epoch 46; iter: 800; batch classifier loss: 0.421085; batch adversarial loss: 0.646213\n",
      "epoch 47; iter: 0; batch classifier loss: 0.334040; batch adversarial loss: 0.615950\n",
      "epoch 47; iter: 200; batch classifier loss: 0.282308; batch adversarial loss: 0.623695\n",
      "epoch 47; iter: 400; batch classifier loss: 0.283859; batch adversarial loss: 0.591290\n",
      "epoch 47; iter: 600; batch classifier loss: 0.304381; batch adversarial loss: 0.574920\n",
      "epoch 47; iter: 800; batch classifier loss: 0.318605; batch adversarial loss: 0.553137\n",
      "epoch 48; iter: 0; batch classifier loss: 0.315072; batch adversarial loss: 0.620160\n",
      "epoch 48; iter: 200; batch classifier loss: 0.284025; batch adversarial loss: 0.584222\n",
      "epoch 48; iter: 400; batch classifier loss: 0.296495; batch adversarial loss: 0.636751\n",
      "epoch 48; iter: 600; batch classifier loss: 0.224557; batch adversarial loss: 0.621463\n",
      "epoch 48; iter: 800; batch classifier loss: 0.383561; batch adversarial loss: 0.667429\n",
      "epoch 49; iter: 0; batch classifier loss: 0.404183; batch adversarial loss: 0.620062\n",
      "epoch 49; iter: 200; batch classifier loss: 0.303204; batch adversarial loss: 0.625394\n",
      "epoch 49; iter: 400; batch classifier loss: 0.326067; batch adversarial loss: 0.566123\n",
      "epoch 49; iter: 600; batch classifier loss: 0.354927; batch adversarial loss: 0.645473\n",
      "epoch 49; iter: 800; batch classifier loss: 0.301648; batch adversarial loss: 0.592250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7fa3a077f610>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)\n",
    "\n",
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Pred: %{x}<br>Truth: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "texttemplate": "%{z}",
         "type": "heatmap",
         "x": [
          "False",
          "True"
         ],
         "xaxis": "x",
         "y": [
          "False",
          "True"
         ],
         "yaxis": "y",
         "z": [
          [
           23704,
           5013
          ],
          [
           929,
           764
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "title": {
          "text": "Pred"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Truth"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"62931a1d-4f88-425d-b98c-ebec48882d65\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"62931a1d-4f88-425d-b98c-ebec48882d65\")) {                    Plotly.newPlot(                        \"62931a1d-4f88-425d-b98c-ebec48882d65\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"texttemplate\":\"%{z}\",\"x\":[\"False\",\"True\"],\"y\":[\"False\",\"True\"],\"z\":[[23704,5013],[929,764]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Pred: %{x}\\u003cbr\\u003eTruth: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Pred\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Truth\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('62931a1d-4f88-425d-b98c-ebec48882d65');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_conf_matrix(dataset_orig_test.convert_to_dataframe()[0]['mortal'], dataset_debiasing_test.convert_to_dataframe()[0]['mortal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.085004\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.014355\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.074816\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.002310\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.823874\n",
      "Test set: Balanced classification accuracy = 0.633825\n",
      "Test set: Disparate impact = 1.087832\n",
      "Test set: Equal opportunity difference = 0.080551\n",
      "Test set: Average odds difference = 0.049111\n",
      "Test set: Theil_index = 0.073639\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.804604\n",
      "Test set: Balanced classification accuracy = 0.638352\n",
      "Test set: Disparate impact = 1.012199\n",
      "Test set: Equal opportunity difference = 0.046064\n",
      "Test set: Average odds difference = 0.026449\n",
      "Test set: Theil_index = 0.075456\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_debiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
